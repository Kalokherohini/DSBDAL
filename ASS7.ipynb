{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c757126a-8c60-4677-8b15-d3f214dd170a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: click in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2023.12.25)\n",
      "Requirement already satisfied: tqdm in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\sujal\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0a84ee33-d172-409f-a542-c5c2e533bc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "64df55ba-b9b9-4886-aae9-43adfe66e75b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\sujal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\sujal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\sujal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sujal\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b47dc221-8be5-4353-a72f-822d2d705b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "text=\"Tockenization is the first step in text analystics. The process of breaking down a text paragraph into smaller chunks such as word of sentences is Tokenization.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "188ac21c-d308-4799-8e63-e730d517b261",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tockenization is the first step in text analystics.', 'The process of breaking down a text paragraph into smaller chunks such as word of sentences is Tokenization.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "tokenized_text = sent_tokenize(text)\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "db166873-5e58-48d9-8729-30092a7871ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Tockenization', 'is', 'the', 'first', 'step', 'in', 'text', 'analystics', '.', 'The', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chunks', 'such', 'as', 'word', 'of', 'sentences', 'is', 'Tokenization', '.']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "tokenized_word=word_tokenize(text)\n",
    "print(tokenized_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c3fa552-2512-4a10-9eba-24050e68e860",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hasn', 'just', 'about', \"mightn't\", 'any', 'with', 'he', 'their', \"hadn't\", 'mightn', 'if', 'have', 'will', 'should', 've', \"you're\", 's', \"you'll\", 'or', 'in', 'his', 'who', \"haven't\", 'can', 'm', 'than', \"don't\", 'of', 'where', 'it', 'off', \"needn't\", \"you'd\", \"you've\", 'are', 'itself', 'to', 'haven', 'its', 'we', 't', 'isn', 'before', \"she's\", 'shouldn', 'do', 'above', \"aren't\", 'whom', 'themselves', 'and', \"shouldn't\", 'having', 'while', \"should've\", \"it's\", 'does', 'no', 'yours', 'once', 'how', 'at', 'doesn', 'our', 'were', 'up', 'own', 'because', 'an', 'your', 'him', 'why', 'mustn', 'until', 'over', 'a', 'but', 'after', 'she', 'been', 'more', 'ma', 'my', 'aren', 'weren', 'her', 'had', 'further', 'for', 'this', 'there', 'against', 'nor', 'most', 'which', 'needn', 'am', 'theirs', \"wasn't\", 'not', 'such', 'shan', 'you', 'did', 'herself', 'same', 'all', 'd', 'y', 'they', 'so', 'under', 'now', 'during', 'didn', \"mustn't\", 'each', 'wasn', 'being', \"didn't\", 'what', 'the', 'be', 'wouldn', \"won't\", \"weren't\", 'between', 'here', \"wouldn't\", 'down', 'when', 'again', 'those', 'both', 'won', 'yourself', 'himself', 'that', 'myself', 'hers', \"that'll\", 'll', 'ours', \"isn't\", 'yourselves', 'ain', 'o', 'was', 'from', 'then', \"hasn't\", 'through', 'ourselves', 'doing', 'hadn', 'has', \"doesn't\", 'them', 'too', 'couldn', 'some', 'very', 'me', 'into', 'other', 'i', 'out', 'as', 'below', 'don', \"shan't\", 'these', \"couldn't\", 'on', 'by', 'only', 're', 'few', 'is'}\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=set(stopwords.words(\"english\"))\n",
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1b1aa78-0c39-4aa1-b384-f8f98f3bf919",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "149483b8-a928-4c95-ac76-2b4a145abd0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized sentence: ['the', 'process', 'of', 'breaking', 'down', 'a', 'text', 'paragraph', 'into', 'smaller', 'chuncks', 'such', 'as', 'word', 'of', 'sentences', 'is', 'tokenization']\n",
      "Filterd sentence: ['process', 'breaking', 'text', 'paragraph', 'smaller', 'chuncks', 'word', 'sentences', 'tokenization']\n"
     ]
    }
   ],
   "source": [
    "text=\"The process of breaking down a text paragraph into smaller chuncks such as word of sentences is tokenization\"\n",
    "text= re.sub('[^a-zA-Z]',' ',text)\n",
    "tokens= word_tokenize(text.lower())\n",
    "filtered_text=[]\n",
    "for w in tokens:\n",
    "    if w not in stop_words:\n",
    "        filtered_text.append(w)\n",
    "print(\"Tokenized sentence:\",tokens)\n",
    "print(\"Filterd sentence:\",filtered_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "19cf3ca9-741c-4076-850d-99d116e50935",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wait\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "e_words=[\"wait\",\"waiting\",\"waited\",\"waits\"]\n",
    "ps=PorterStemmer()\n",
    "for w in e_words:\n",
    "    rootword=ps.stem(w)\n",
    "print(rootword)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e881b773-024b-42cc-87e0-de2b9d921961",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "studies study\n",
      "studying studying\n",
      "cries cry\n",
      "cry cry\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "text = \"studies studying cries cry\"\n",
    "tokenization = nltk.word_tokenize(text)\n",
    "for w in tokenization:\n",
    "    print(w,wordnet_lemmatizer.lemmatize(w))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "19a0dda7-1274-4e81-9619-d341f9b6410e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT')]\n",
      "[('black', 'JJ')]\n",
      "[('shirt', 'NN')]\n",
      "[('fit', 'NN')]\n",
      "[('him', 'PRP')]\n",
      "[('perfectly', 'RB')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"The black shirt fit him perfectly\"\n",
    "words=word_tokenize(data)\n",
    "for word in words:\n",
    "    print(nltk.pos_tag([word]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4857e917-c749-4ccf-91f2-319ddb49cf4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "paragraph = \"\"\"\n",
    "I am scraping the New york Times webpages to do some natural\n",
    "language processing on it, I want to split the webpage into paragra\n",
    "phs when using corpus in order to do frequency counts on words that\n",
    "appear in paragraphs which also contain key words or phrases \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0ce6855a-beb9-4fbd-a0d5-00c7668b3c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b8e9a7ec-6091-4a48-be6c-7c1b40fbd5f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nI am scraping the New york Times webpages to do some natural\\nlanguage processing on it, I want to split the webpage into paragra\\nphs when using corpus in order to do frequency counts on words that\\nappear in paragraphs which also contain key words or phrases']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wn=WordNetLemmatizer()\n",
    "sentences=nltk.sent_tokenize(paragraph)\n",
    "sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71773625-e0ca-4a99-bcdb-3e1e4b204a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.25607376 0.12803688 0.25607376 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.25607376 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.12803688 0.12803688 0.12803688 0.25607376 0.12803688 0.38411064\n",
      "  0.12803688 0.12803688 0.12803688 0.12803688 0.12803688 0.12803688\n",
      "  0.25607376 0.12803688]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf=TfidfVectorizer()\n",
    "x=tf.fit_transform(sentences).toarray()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "428c72fd-11b7-4555-b717-0ba062ebf979",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624108bc-da80-436d-97ea-630ac556cbd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
